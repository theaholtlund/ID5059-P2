{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64e4658a",
   "metadata": {},
   "source": [
    "# 1. Frame the Data\n",
    "\n",
    "Before doing any further examination, processing or training on the dataset, it is important to frame the data. This process is concerned with determining what the outcome and goal of the model looks like, defining what the output of the model should be, and understanding what a successful solution of the problem would look like (Google Developers, 2023). For this particular scenario, the ideal outcome is a model that can detect fraud, by determining wehether a credit card transaction is genuine or fraudulent. In terms of outputs, the model will be dealing with a binary classification problem, genuine or fraudulent transaction represented by a 0 or a 1, and will therefore have 2 outputs. For the success metrics, we can consider accuracy, precision, recall and F1 score. We will prioritise recall over the other metrics because it is more important that all fraudulent transactions are classified as such than that flagged transactions are actually fraudulent: most people would rather need to authenticate some genuine transcations than have their money stolen. Obviously, however, if precision is too low then people are likely to assume that flagged transactions are incorrectly flagged and verify them without checking out of laziness, so we want a precision of at least 0.50.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bce7203",
   "metadata": {},
   "source": [
    "# 2. Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data models and training process\n",
    "import sys\n",
    "!{sys.executable} - m pip install numpy pandas matplotlib scikit-learn seaborn imblearn plotnine | grep - v 'already satisfied'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import imblearn\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import plotnine as p9\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from math import log, exp\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, precision_recall_curve, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad23977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the dataset\n",
    "# Read train csv, check the first ten lines of dataset\n",
    "credit_card = pandas.read_csv(\"../data/train.csv\")\n",
    "credit_card.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79620bf6",
   "metadata": {},
   "source": [
    "# 3. Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the relationship between any feature and the response variable\n",
    "attributes = [\"V17\",\n",
    "              \"Class\",]\n",
    "scatter_matrix(credit_card[attributes],figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the DataFrame\n",
    "credit_card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the information on the DataFrame\n",
    "# The results show the data is in the dtyp float64(30), int64(2)\n",
    "credit_card.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some additional information from the description\n",
    "credit_card.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the correlations in the DataFrame\n",
    "credit_card_correlations = credit_card.corr() \n",
    "credit_card_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_correlations[\"Class\"].sort_values(ascending=False)\n",
    "       \n",
    "# greater than 0.02: Positive correlation: V4,V18 \n",
    "#                    Negative correlation: V3,V1,V14,V8,V7,V12\n",
    "#\n",
    "#         0.01-0.02: Positive correlation: V28,V20,V11,V9,V17,Amount,V2,V5\n",
    "#                    Negative correlation: V10,V26,V23 \n",
    "#\n",
    "# might delete the other features less than 0.01 or 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f664201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closer look at one particular attribute from the DataFrame\n",
    "credit_card_v04 = credit_card[[\"V4\"]] \n",
    "credit_card_v04 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the \"Class\" attribute\n",
    "credit_card_class = credit_card[[\"Class\"]] \n",
    "credit_card_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card.groupby(\"Class\").size()\n",
    "\n",
    "# 469    cases of fraudulent\n",
    "# 218660 cases of genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7232622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a figure to show the results, keeping 5 decimal points\n",
    "# Description of procedure found in external resource (Kaggle, 2017)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.pie(credit_card.Class.value_counts(),autopct='%0.5f%%', labels=['0','1'], colors=['b','r'])\n",
    "\n",
    "ax.set_title(\"pie\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e434ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the \"Amount\" attribute part\n",
    "fig, (ax_fraudulent, ax_genuine) = plt.subplots(2, 1, figsize=(12,10))  \n",
    "\n",
    "# Subplot creates a figure and a grid of subplots\n",
    "ax_fraudulent.hist(credit_card.Amount[credit_card[\"Class\"] == 1],  color = 'r' ,bins = 50, alpha=0.7)\n",
    "ax_fraudulent.set_title('Fraudulent Transactions')\n",
    "\n",
    "ax_genuine.hist(credit_card.Amount[credit_card[\"Class\"] == 0], color = 'b' ,bins = 50, alpha=0.7)  \n",
    "ax_genuine.set_title('Genuine Transactions')\n",
    "\n",
    "plt.xlabel('Transaction Amount')  \n",
    "plt.ylabel('Number of Transactions')  \n",
    "plt.yscale('log')          \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de39e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the \"Time\" attribute part\n",
    "# This dataset presents transactions that occurred in two days\n",
    "\n",
    "fig, (ax_fraudulent, ax_genuine) = plt.subplots(2, 1, figsize=(12,10))  \n",
    "\n",
    "ax_fraudulent.hist(credit_card.Time[credit_card[\"Class\"] == 1],  color = 'r' ,bins = 50, alpha=0.7)\n",
    "ax_fraudulent.set_title('Fraudulent Transactions')\n",
    "\n",
    "ax_genuine.hist(credit_card.Time[credit_card[\"Class\"] == 0], color = 'b' ,bins = 50, alpha=0.7) \n",
    "ax_genuine.set_title('Genuine Transactions')\n",
    "\n",
    "plt.xlabel('Amount of Transactions')  \n",
    "plt.ylabel('Number of Transactions')  \n",
    "plt.yscale('log')                    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the amount of fraud cases\n",
    "credit_card = pandas.read_csv(\"../data/train.csv\")\n",
    "\n",
    "credit_card_fraud = credit_card.loc[credit_card[\"Class\"] == 1]  \n",
    "credit_card_fraud\n",
    "\n",
    "credit_card_sort = credit_card_fraud.sort_values(by = 'Amount')\n",
    "credit_card_sort\n",
    "\n",
    "\n",
    "#relatively large transactions in fraudulent cases:2096.00\n",
    "#                                                  2500.00\n",
    "#                                                  2727.18\n",
    "#                                                  3000.00\n",
    "#                                                  4471.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c10b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the amount of normal cases\n",
    "credit_card = pandas.read_csv(\"../data/train.csv\")\n",
    "\n",
    "credit_card_fraud = credit_card.loc[credit_card[\"Class\"] == 0]  \n",
    "credit_card_fraud\n",
    "\n",
    "credit_card_sort = credit_card_fraud.sort_values(by = 'Amount')\n",
    "credit_card_sort\n",
    "\n",
    "# relatively large transactions in genuine cases:4610.36\n",
    "#                                                4632.00\n",
    "#                                                2727.18\n",
    "#                                                4669.77\n",
    "#                                                6513.35\n",
    "#                                                7475.00"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0b1aa8a",
   "metadata": {},
   "source": [
    "We plot the distributions for each feature, split by class (positive is red, negative is blue). Features with similar distributions for positive and negative classes are unlikely to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44998d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of each feature \n",
    "# One might want to delete features that are not significantly different between the fraudulent and genuine distribution\n",
    "# Description of procedure found in external resource (Kaggle, 2017)\n",
    "\n",
    "gs = gs.GridSpec(31, 1)             #GridSpec: a more general subplot layout\n",
    "\n",
    "plt.figure(figsize=(10,31*5))\n",
    "\n",
    "for i, col in enumerate(credit_card[credit_card.iloc[:,0:31].columns]):   \n",
    "#Except for line 32, which is the Class attribute\n",
    "    \n",
    "    ax1 = plt.subplot(gs[i])\n",
    "    \n",
    "    sns.distplot(credit_card[col][credit_card.Class == 1], bins=50, color='r',)\n",
    "    sns.distplot(credit_card[col][credit_card.Class == 0], bins=50, color='b',)\n",
    "    \n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_title('feature ' + str(col))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e06d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram for each feature\n",
    "credit_card.hist(bins = 50, figsize = (30,50))\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b029aae",
   "metadata": {},
   "source": [
    "# 4. Prepare the Data\n",
    "The next step in the process for a machine learning problem like this would be to prepare the data. Important steps to consider here are what columns to drop from the dataset, how to deal with missing values and how to convert attributes into numerical values. Depending on the dataset, there might also be a need for processing raw data, including values, symbols or syntax a machine learning model would not be able to process (Brownlee, 2020a). Considering how all values of the dataset are numerical, with no categories, this data preparation will not be necessary to consider for the imputation methods. However, moving on to the next step, preparation work must be performed before various models can be explored and trained on the dataset.\n",
    "\n",
    "Besides assessing some imputation methods, to determine the most effective way of replacing missing values in the dataset, it is necessry to drop some attributes from the dataset before using it to train machine learning models. We therefore drop the \"Class\" attribute, as this is the target value we are trying to predict, and it therefore should not be included in the training set. The \"id\" is also not indicative of any meaningful data we will be working with, as it is just a ascending number to identify the transaction for the specfic row.\n",
    "\n",
    "Additionally, it is important to consider feature scaling. Machine learning algorithms generally do not perform well on attributes that are considerably different in scale, which is the case for the credit card dataset. While the values for the attributes V1 to V28 have similar scale, the numerical value of the \"Time\" and \"Amount\" attributes vary drastically in scale from these. While the attributes V1 to V28 all have a mean around 0, and a standard deviation between 0 and 1, the \"Time\" attribute has a mean of 62377.42 and a standard deviation of 25620.35. Similarly, the \"Amount\" attribute have quite different values for these metrics, with a mean of 66.36 and a standard deviation of 150.80. As such, these attributes should be scaled.\n",
    "\n",
    "Considering how several opther attributes of the dataset deals with numerical values in the range of 0 to 1, normalisation would be an appropriate scaling method for these attributes. Using normalisation would also be the better choice considering how the data does not have a normal distribution, and it would ensure the prepared data is compatible with models that expect a value between 0 and 1, such as neural networks (GÃ©ron, 2019).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fad68c89",
   "metadata": {},
   "source": [
    "## 4.1 Splitting the Datasets\n",
    "We know from our data exploration that the two classes are very unbalanced, and that we have a large set of features, which may not all be useful. We therefore consider alternate datasets that could be better for training models.\n",
    "\n",
    "### 4.1.1 Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the original dataset again\n",
    "credit_card"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32a55d04",
   "metadata": {},
   "source": [
    "### 4.1.2 Dropping non-correlated features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_correlations[\"Class\"].sort_values(ascending=False)\n",
    "       \n",
    "# greater than 0.02: Positive correlation: V4,V18 \n",
    "#                    Negative correlation: V3,V1,V14,V8,V7,V12\n",
    "#\n",
    "#         0.01-0.02: Positive correlation: V28,V20,V11,V9,V17,Amount,V2,V5\n",
    "#                    Negative correlation: V10,V26,V23 \n",
    "#\n",
    "# might delete the other features less than 0.01 or 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ade75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the less relevant features according to the score\n",
    "# score > 0.2\n",
    "\n",
    "uncorr_features = [\n",
    "    'Class',\n",
    "    'id',\n",
    "    'Time',\n",
    "\n",
    "    'V28',\n",
    "    'V20',\n",
    "    'V11',\n",
    "    'V9',\n",
    "    'V17',\n",
    "    'V2',\n",
    "    'V5',\n",
    "    'V21',\n",
    "    'V15',\n",
    "    'V13',\n",
    "    'V22',\n",
    "    'V25',\n",
    "    'V6',\n",
    "    'V24',\n",
    "    'V16',\n",
    "    'V19',\n",
    "    'V23',\n",
    "    'V26',\n",
    "    'V10',]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89356b3b",
   "metadata": {},
   "source": [
    "### 4.1.3 Undersample\n",
    "We can undersample from the negative class to get a more even distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random undersampling to balance the \"Class\" distribution, using a 0.2 strategy\n",
    "\n",
    "X = credit_card.drop(columns=[\"Class\"])\n",
    "y = credit_card[[\"Class\"]]\n",
    "\n",
    "\n",
    "# define undersample strategy\n",
    "# there are 218,660 examples in the majority class and 469 examples in the minority class\n",
    "# set sampling_strategy to 0.2 because 469/2345 = 0.2\n",
    "# then the majority of classes in the transformed data set will have 2345 examples\n",
    "\n",
    "\n",
    "#undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "undersample_02 = RandomUnderSampler(sampling_strategy=0.2)\n",
    "\n",
    "credit_card_X_under, credit_card_labels_under = undersample_02.fit_resample(X, y)\n",
    "\n",
    "del X, y, undersample_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_X_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_labels_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_labels_under.groupby(\"Class\").size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3bebf85",
   "metadata": {},
   "source": [
    "### 4.1.4 Oversample\n",
    "We can alternatively oversample from the positive class to get a more even distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversampling to balance the class distribution\n",
    "\n",
    "X = credit_card.drop(columns=[\"Class\"])\n",
    "y = credit_card[[\"Class\"]]\n",
    "\n",
    "# Define the oversample strategy  \n",
    "oversample_025 = RandomOverSampler(sampling_strategy=0.25)\n",
    "\n",
    "# Fit the results to the oversampling strategy\n",
    "credit_card_X_over, credit_card_labels_over = oversample_025 .fit_resample(X, y)\n",
    "\n",
    "del X, y, oversample_025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a475d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_X_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f228d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_labels_over.groupby(\"Class\").size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a548cdb6",
   "metadata": {},
   "source": [
    "#### 4.1.5 Dropping non-important features (Dimension Reduction)\n",
    "\n",
    "We have about 30 features. It should be possible to use Principle Component Analysis (PCA) to reduce the number of features, either by collapsing them together or even removing unimportant features completely. \n",
    "\n",
    "Before applying PCA, we need to scale our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a918ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def trial_PCA(n_components, verbose=True, plot_graph=False): \n",
    "    \"\"\"Applies PCA to the credit_card DataFrame and prints the amount of captured variance.\n",
    "       Optionally returns 2-component outputs on a graph, otherwise returns the percentage of lost variance.\"\"\"\n",
    "    # split data\n",
    "    credit_card_x = credit_card.drop(columns=[\"id\", \"Class\"])\n",
    "    credit_card_y = credit_card[\"Class\"] # for plotting \n",
    "\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(credit_card_x)\n",
    "    credit_cared_x_scaled = scaler.transform(credit_card_x)\n",
    "\n",
    "    # apply pca\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(credit_cared_x_scaled)\n",
    "    credit_card_pca_x = pandas.DataFrame(pca.transform(credit_cared_x_scaled)) # return value needed for plotting \n",
    "\n",
    "    # see how much variance we lost \n",
    "    print(f\"lost {(1 - pca.explained_variance_ratio_.sum()) *100}% of the variance with {len(pca.components_)} components\")\n",
    "\n",
    "    # see how much variance was captured by each dimension \n",
    "    if verbose: \n",
    "        for i in range(len(pca.components_)): \n",
    "            print(f\"PCA feature {i} captured {(pca.explained_variance_ratio_[i]) * 100}% of the variance\")\n",
    "\n",
    "    if (plot_graph and n_components == 2): \n",
    "        return p9.ggplot(credit_card_pca_x) + p9.geom_point(p9.aes(x=credit_card_pca_x.columns[0], y=credit_card_pca_x.columns[1], color=[str(y) for y  in credit_card_y]))\n",
    "    else: \n",
    "        return (1 - pca.explained_variance_ratio_.sum()) *100\n",
    "\n",
    "trial_PCA(2, plot_graph=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "260c45ee",
   "metadata": {},
   "source": [
    "Clearly, collapsing down to 2 features loses far too much of the variance. We will try with n_components=10, which still significantly reduces our dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ee659",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_PCA(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7170c15",
   "metadata": {},
   "source": [
    "This is still too much variance loss, so we will see what number of components is needed to capture 95% of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_PCA(0.95, verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429f5149",
   "metadata": {},
   "source": [
    "26 components is not much of a feature reduction, so we will investigate how the loss varies with the number of principal components in the hopes of finding some number of components that meaningfully reduce the dimensions of our search space without losing too much variance (the elbow of the curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = []\n",
    "lost_variances = []\n",
    "start = 2\n",
    "stop = 30\n",
    "for i in range(start, stop):\n",
    "    n_components.append(i)\n",
    "    lost_variances.append(trial_PCA(i, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc633ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.scatter(x=n_components, y=lost_variances)\n",
    "plt.xticks(range(start, stop))\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"Lost variance %\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d37df7a9",
   "metadata": {},
   "source": [
    "The above curve does not have an obvious elbow: increasing the number of principal components (n) pretty consistently increases the captured variance by 2-3% for n >= 15, up to 5-6% for n <= 7. This halves the gradient, but the change is quite smooth, so picking a number to use is difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "del n_components, lost_variances, start, stop, i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0cdc855",
   "metadata": {},
   "source": [
    "Finally, we consider which features of credit_card explain the most variance for n = 26 across all principal components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "credit_card_x = credit_card.drop(columns=[\"id\", \"Class\"])\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(credit_card_x)\n",
    "credit_cared_x_scaled = scaler.transform(credit_card_x)\n",
    "\n",
    "# Apply pca\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(credit_cared_x_scaled)\n",
    "\n",
    "feature_variance_sum = {}\n",
    "for column in credit_card_x.columns:\n",
    "    feature_variance_sum[column] = 0\n",
    "\n",
    "for p_component in pca.components_:\n",
    "    score_list = [abs(score) for score in list(p_component)]\n",
    "    for i in range(len(score_list)):\n",
    "        feature_variance_sum[credit_card_x.columns[i]] += score_list[i]\n",
    "\n",
    "feature_variance_sum_list = [(k, v / len(pca.components_)) for k, v in feature_variance_sum.items()]\n",
    "feature_variance_sum_list.sort(reverse=True, key=lambda tup: tup[1])\n",
    "print(\"\\n\".join([str(item) for item in feature_variance_sum_list]))\n",
    "\n",
    "del credit_card_x, scaler, credit_cared_x_scaled, pca, feature_variance_sum, feature_variance_sum_list, p_component, column, score_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d765a530",
   "metadata": {},
   "source": [
    "We can see that V4, V21, V8, V25, V2, V22 and V26 all explain at least 15% of the variance per component on average. Given that PCA loses a lot of the variance for low values of n, we could take the more drastic measure of simply dropping all of the other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance_features = [\"V4\",\n",
    "                         \"V21\", \"V8\", \"V25\", \"V2\", \"V22\", \"V26\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf956e5",
   "metadata": {},
   "source": [
    "### 4.1.6 Feature Deletion\n",
    "We can alternatively reduce our feature set by dropping features that had similar distributions in both classes, as explored earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_distribution_features = [\n",
    "    'Class',\n",
    "    'id',\n",
    "    'Time',\n",
    "\n",
    "    'V5',\n",
    "    'V6',\n",
    "    'V7',\n",
    "    'V8',\n",
    "    'V12',\n",
    "    'V13',\n",
    "    'V15',\n",
    "    'V16',\n",
    "    'V20',\n",
    "    'V21',\n",
    "    'V22',\n",
    "    'V23',\n",
    "    'V25',\n",
    "    'V27',\n",
    "    'V28',\n",
    "\n",
    "    'Amount',]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23fee87d",
   "metadata": {},
   "source": [
    "## 4.2 Data Cleaning\n",
    "\n",
    "Aside from the different data sets, we should drop the columns that we do not think will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom transformer\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/68402691/adding-dropping-column-instance-into-a-pipeline\n",
    "\n",
    "class ColumnRemover():\n",
    "   def __init__(self, columns):\n",
    "    self.columns = columns\n",
    "     \n",
    "   def transform(self, X, y=None):\n",
    "    return X.drop(self.columns, axis=1)\n",
    "\n",
    "   def fit(self, X, y=None):\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for cleaning the data\n",
    "# Source: # Source: https://stackoverflow.com/questions/68402691/adding-dropping-column-instance-into-a-pipeline\n",
    "\n",
    "# Dropping the id, time, and class columns, as described above\n",
    "pipeline = Pipeline([\n",
    "    (\"columnDropper\", ColumnRemover(['id', 'Class', 'Time'])),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Apply the pipeline to dataframe\n",
    "\n",
    "credit_card_transformed = pipeline.fit_transform(credit_card)\n",
    "credit_card_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28054cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correlation between features and the target which is the class attribute\n",
    "credit_card_correlations = credit_card.corr() \n",
    "credit_card_correlations[\"Class\"].sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edaf5eed",
   "metadata": {},
   "source": [
    "## 4.3 Imputing Missing Data\n",
    "\n",
    "### 4.3.1 Randomly Removing Data\n",
    "The training data has no missing values, so before we can test imputation methods we need a way of deleting values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8114d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are no missing values in the training set, we need to create some\n",
    "# The procedure for how this can be done was found in an external resource (Stack Overflow, 2020)\n",
    "# This can be done by defining a value for the percentage of values that should be missing, and then removing these\n",
    "\n",
    "def remove_random_values(df: pandas.DataFrame, missing_percentage=0.05, random_seed=42):\n",
    "    \"\"\"Returns a copy of a DataFrame with some missing values.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    df_copy = df.copy()\n",
    "    # Generate some missing values for the training set, and then replace these with NaN (GeeksforGeeks, 2022)\n",
    "    # Perform same operations to create missing values as was done for the median imputation method\n",
    "    missing_values = np.zeros(df_copy.shape, dtype=bool)\n",
    "    missing_values[:, 1:] = np.random.choice([True, False], size=(\n",
    "        df_copy.shape[0], df_copy.shape[1]-1), p=[missing_percentage, 1-missing_percentage])\n",
    "    df_copy[missing_values] = np.nan\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable for dataset with missing values\n",
    "credit_card_missing = remove_random_values(credit_card)\n",
    "\n",
    "# See how many null values we are working with now\n",
    "credit_card_missing.info()\n",
    "\n",
    "del credit_card_missing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2109baf1",
   "metadata": {},
   "source": [
    "### 4.3.2 Imputing the Mean \n",
    "Our first imputation strategy is to use the mean value. The mean imputation method is a common strategy for replacing missing values, assigning the mean in their place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91742dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having created the missing values, we can now use the mean imputation method\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "credit_card_missing_values = remove_random_values(credit_card)\n",
    "imputed_mean_arr = mean_imputer.fit_transform(credit_card_missing_values)\n",
    "\n",
    "# Converting the array for the imputed mean into DataFrame from an array \n",
    "imputed_mean_df = pandas.DataFrame(imputed_mean_arr, columns=credit_card.columns)\n",
    "\n",
    "imputed_mean_df.info()\n",
    "\n",
    "del mean_imputer, credit_card_missing_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45cdaa71",
   "metadata": {},
   "source": [
    "Scikit-learn's SimpleImputer likely works as expected, but we can now evaluate the mean strategy. First we consider its impact on the mean value for each column. We expected that it will be preserved, but we are now able to check this in our code to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage difference of the mean using the mean imputation method\n",
    "percentage_diff_means = [(round(abs(imputed_mean - original_mean)/original_mean * 100, 2)) for (original_mean, imputed_mean)\n",
    " in zip(list(np.mean(credit_card, axis=0)), list(np.mean(imputed_mean_df, axis=0)))]\n",
    "\n",
    "# We exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the mean for each column is {}%\".format(np.mean(percentage_diff_means[2:])))\n",
    "\n",
    "del percentage_diff_means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89f96e53",
   "metadata": {},
   "source": [
    "As expected, this is a very minor impact. As such, using the mean prediactably preserves the value of the mean.\n",
    "\n",
    "Next, we can consider the impact on the median value, when using the mean imputation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_diff_medians = []\n",
    "for (original_median, imputed_median) in zip(list(np.median(credit_card.drop(columns=[\"Class\"]), axis=0)), list(np.median(imputed_mean_df.drop(columns=[\"Class\"]), axis=0))):\n",
    "    try:\n",
    "        percentage_diff_medians.append(round(exp(\n",
    "            log(abs(imputed_median - original_median)) - log(abs(original_median))) * 100, 2))\n",
    "        if (percentage_diff_medians[-1] > 0.5):\n",
    "            # print(\"Big difference. Original median: {}, Imputed median: {}\".format(original_median, imputed_median))\n",
    "            pass \n",
    "    except:\n",
    "        percentage_diff_medians.append(0)\n",
    "\n",
    "\n",
    "# We exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the median for each column is {}%\".format(\n",
    "    np.mean(percentage_diff_medians[2:])))\n",
    "\n",
    "del percentage_diff_medians"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a22c453",
   "metadata": {},
   "source": [
    "This is a much bigger impact than on the mean value. So far the mean imputation method, in calculating the mean impact on the median for each column, is not performing as well as one might have hoped for, considering how it has a 50.16% difference compared to the original dataset. Next, we can compare the variance between the original dataset and the mean imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage difference in variance between the original and imputed data\n",
    "percentage_diff_variances = [(round(abs(imputed_variance - original_variance)/original_variance * 100, 2)) for (original_variance, imputed_variance)\n",
    " in zip(list(np.var(credit_card, axis=0)), list(np.var(imputed_mean_df, axis=0)))]\n",
    "\n",
    "# We exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the variance for each column is {}%\".format(np.mean(percentage_diff_variances[2:])))\n",
    "\n",
    "del percentage_diff_variances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b04fb36",
   "metadata": {},
   "source": [
    "As we can see, the variance for each column is 5.04%, which is not a significant difference, especially considering the size of the dataset. Next, We can explore the distributions graphically for some columns, shown below. Next, we can create a plot where both median outputs are displayed on the same axis, so that they can easily be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at the median and mean for the imputed mean\n",
    "print(np.median(credit_card[[\"V1\"]]))\n",
    "print(np.median(imputed_mean_df[[\"V1\"]]))\n",
    "\n",
    "print(np.mean(credit_card[[\"V1\"]]))\n",
    "print(np.mean(imputed_mean_df[[\"V1\"]]))\n",
    "\n",
    "# Explore the distribution graphically for some given columns\n",
    "fig, ax = plt.subplots()\n",
    "# Plot a scatter plot for the credit card data\n",
    "credit_card.plot.scatter(ax=ax, x=\"id\", y=\"V1\", color='red', label='Credit Card')\n",
    "# Plot a scatter plot for the credit card data with the imputed mean\n",
    "imputed_mean_df.plot.scatter(ax=ax, x=\"id\", y=\"V1\", color='orange', label='Imputed Mean')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f20a5584",
   "metadata": {},
   "source": [
    "Having compared some key metrics from the original dataset to the corresponding values from the dataset imputed with the mean, it becomes evident that the mean might be an acceptable alternative in replacing missing values, though it is worth noting that it does not perform well for the mean impact on the median for each column. However, when printing out the difference for each column for the original dataset and the dataset using the mean imputation method, we can perceive that this is because of the one outlier dealing with larger numbers, as the other inputs are performing relatively well. Furthermore, the mean for a samle attribute \"V1\" of the original dataset is almost identical after mean imputation, with a value of 0.096008 for the original dataset and 0.09544 for the imputed mean dataset. This further confirms that replacing missing values with the mean will yield a result that closely resembles the original data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "486305c4",
   "metadata": {},
   "source": [
    "### 4.3.3 Imputing the Median\n",
    "\n",
    "Next, we decided to use the median as our second imputation strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419039de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating the missing values, we applied the median imputation method\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "credit_card_missing_values = remove_random_values(credit_card)\n",
    "imputed_median_arr = median_imputer.fit_transform(credit_card_missing_values)\n",
    "\n",
    "# Converting the array for the imputed median into DataFrame from an array \n",
    "imputed_median_df = pandas.DataFrame(imputed_median_arr, columns=credit_card.columns)\n",
    "\n",
    "imputed_median_df.info()\n",
    "\n",
    "del median_imputer, credit_card_missing_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10a98eaa",
   "metadata": {},
   "source": [
    "Next, the median imputation strategy was evaluated by comparing the means of the imputed median values to the actual data's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_diff_means = [(round(abs(imputed_median - original_median)/original_median * 100, 2)) for (original_mean, imputed_mean)\n",
    " in zip(list(np.mean(credit_card, axis=0)), list(np.mean(imputed_median_df, axis=0)))]\n",
    "\n",
    "# we exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the mean for each column is {}%\".format(np.mean(percentage_diff_means[2:])))\n",
    "\n",
    "del percentage_diff_means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b340f645",
   "metadata": {},
   "source": [
    "14.16% is a significant difference and indicates that the median may not be the most effective for predictably preserving the value of the means. It is also significantly higher than the results from the mean imputation. This could indicate that the median imputation biases the mean of the dataset. Next, the impact on the median value was evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percentage_diff_medians = []\n",
    "for (original_median, imputed_median) in zip(list(np.median(credit_card.drop(columns=[\"Class\"]), axis=0)), list(np.median(imputed_median_df.drop(columns=[\"Class\"]), axis=0))):\n",
    "    try:\n",
    "        percentage_diff_medians.append(round(exp(\n",
    "            log(abs(imputed_median - original_median)) - log(abs(original_median))) * 100, 2))\n",
    "        if (percentage_diff_medians[-1] > 0.5):\n",
    "            # print(\"big difference, original median: {}, imputed median: {}\".format(original_median, imputed_median))\n",
    "            pass \n",
    "    except:\n",
    "        percentage_diff_medians.append(0)\n",
    "\n",
    "\n",
    "# we exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the median for each column is {}%\".format(\n",
    "    np.mean(percentage_diff_medians[2:])))\n",
    "\n",
    "del percentage_diff_medians"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "258090d4",
   "metadata": {},
   "source": [
    "It appears that the median imputation works significantly better when predicting the missing median values than the mean imputation method, given it only has a 0.596% difference between the median imputed values and the median actual values. Then, the variance between the original data and the median imputed data were compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f40bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage difference in variance between the original and imputed data\n",
    "percentage_diff_variances = [(round(abs(imputed_variance - original_variance)/original_variance * 100, 2)) for (original_variance, imputed_variance)\n",
    " in zip(list(np.var(credit_card, axis=0)), list(np.var(imputed_median_df, axis=0)))]\n",
    "\n",
    "# We exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the variance for each column is {}%\".format(np.mean(percentage_diff_variances[2:])))\n",
    "\n",
    "del percentage_diff_variances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88f45bee",
   "metadata": {},
   "source": [
    "Given the variance is less than 5%, this is considered a small significance, especially when considering the size of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at the median and mean for the imputed median\n",
    "print(np.median(credit_card[[\"V1\"]]))\n",
    "print(np.median(imputed_median_df[[\"V1\"]]))\n",
    "\n",
    "print(np.mean(credit_card[[\"V1\"]]))\n",
    "print(np.mean(imputed_median_df[[\"V1\"]]))\n",
    "\n",
    "# Explore the distribution graphically for some given columns\n",
    "fig, ax = plt.subplots()\n",
    "# Plot a scatter plot for the credit card data\n",
    "credit_card.plot.scatter(ax=ax, x=\"id\", y=\"V1\", color='black', label='Credit Card')\n",
    "# Plot a scatter plot for the credit card data with the imputed median\n",
    "imputed_median_df.plot.scatter(ax=ax, x=\"id\", y=\"V1\", color='darkgrey', label='Imputed Median')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c0908f6",
   "metadata": {},
   "source": [
    "Based on the findings above, it appears that the median imputation method is a relatively good alternative for replacing missing values in the dataset. So far, it performs better than the mean imputation method in all aspects except from the mean impact on the mean for each column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a55a233",
   "metadata": {},
   "source": [
    "### 4.3.4 Imputing from a Binned Frequency Distribution\n",
    "\n",
    "Our third method is more (probably over-) complicated: instead of simply using the median or mean value, we build a frequency distribution of the known values and then replace missing values with (weighted) random values based on that distribution.\n",
    "\n",
    "\n",
    "We first investigate if there are enough non-unique values to sensibly group in a frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_header = \"V7\"\n",
    "values = list(credit_card[column_header])\n",
    "values.sort()\n",
    "print(\"the range of values in data[{}] is {} to {}\".format(\n",
    "    column_header, values[0], values[-1]))\n",
    "vals = list(set(values))\n",
    "print(\"there are {} values in data[{}], of which {} are unique\".format(\n",
    "    len(values), column_header, len(vals)))\n",
    "\n",
    "del column_header, values, vals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1debb568",
   "metadata": {},
   "source": [
    "Most values are unique, so a basic frequency distribution will not be helpful. We can instead group values into a manageable number of bins, create a frequency distribution of those bins, and use the middle values of those bins for our imputation. \n",
    "\n",
    "Our next task is therefore to generate a binned frequency distribution. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aaafac6",
   "metadata": {},
   "source": [
    "#### 4.3.4.1 Building a Binned Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632cff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fq(data: pandas.DataFrame, column_header: str, max_bins: int, debugging=False):\n",
    "    \"\"\"Creates a binned frequency distribution of a given DataFrame's column, using middle values for each bin.\"\"\"\n",
    "    MIN_FREQ = float(1.0e-9)\n",
    "\n",
    "    values = list(data[column_header])\n",
    "    # remove missing values\n",
    "    values = list(filter(lambda x: not np.isnan(x), values))\n",
    "    values.sort()\n",
    "\n",
    "    # ensure that there are not more bins than unique values \n",
    "    max_bins = min(max_bins, len(set(values)))\n",
    "    if debugging:\n",
    "        print(\"get_fq(): max_bins: {}\".format(max_bins))\n",
    "\n",
    "    # find range of values, and corresponding width of initial bins\n",
    "    val_range = abs(values[0] - values[-1])\n",
    "    bin_size = val_range / max_bins\n",
    "    if debugging:\n",
    "        print(\"get_fq(): val_range: {}\".format(val_range))\n",
    "        print(\"get_fq(): bin_size: {}\".format(bin_size))\n",
    "\n",
    "    # find the frequencies of each bin\n",
    "    freqs = [0]\n",
    "    bin_end = values[0] + bin_size\n",
    "    for value in values:\n",
    "        while value > bin_end:\n",
    "            bin_end += bin_size\n",
    "            freqs.append(0)\n",
    "        freqs[-1] += 1\n",
    "    freqs = list(map(lambda x: x/len(values), freqs))\n",
    "    if debugging:\n",
    "        print(\"get_fq(): sum of freqs (should be ~1.0): {}\".format(sum(freqs)))\n",
    "        print(\"get_fq(): sum of freqs below {}: {}\".format(\n",
    "            MIN_FREQ, list(map(lambda x: x < MIN_FREQ, freqs)).count(True)))\n",
    "        print(\"get_fq(): len(freqs) = {}\".format(len(freqs)))\n",
    "\n",
    "    # create a collection of the bins that have frequencies of at least MIN_FREQ, and their corresponding middle values\n",
    "    binned_freq_dist = [[0]]\n",
    "    curr_bin_freq = 0\n",
    "    start = stop = values[0]\n",
    "    for freq in freqs:\n",
    "        curr_bin_freq += freq\n",
    "        stop += bin_size\n",
    "        if (curr_bin_freq > MIN_FREQ):\n",
    "            if debugging:\n",
    "                print(\"get_fq(): bin{index}{{range:{start},{stop} mid:{mid} curr_freq:{curr_freq} freq:{freq}}}\".format(\n",
    "                    index=len(binned_freq_dist), start=start, stop=stop, mid=(start+stop)/2, curr_freq=curr_bin_freq, freq=binned_freq_dist[-1][0] + curr_bin_freq\n",
    "                ))\n",
    "            binned_freq_dist[-1] = [binned_freq_dist[-1][0] + curr_bin_freq,\n",
    "                                    (start + stop) / 2]\n",
    "            binned_freq_dist.append([binned_freq_dist[-1][0]])\n",
    "            curr_bin_freq = 0\n",
    "            start = stop\n",
    "    # remove the last item from the list, which will just be [1]\n",
    "    del binned_freq_dist[-1]\n",
    "    if debugging:\n",
    "        print(\"get_fq(): len(binned_freq_dist) = {}\".format(len(binned_freq_dist)))\n",
    "\n",
    "    return binned_freq_dist\n",
    "\n",
    "\n",
    "def get_val_from_fq(fq: list, rand_flt: float):\n",
    "    \"\"\"Returns a value from a binned frequency distribution corresponding to a given random float between 0 and 1\"\"\"\n",
    "    if rand_flt < 0 or rand_flt > 1:\n",
    "        raise ValueError(\"rand_flt must be between 0 and 1\")\n",
    "    for [freq, value] in fq:\n",
    "        if rand_flt <= freq:\n",
    "            return value\n",
    "    # catch rand_flt == 1 in the case that the final frequency is 0.999999999\n",
    "    return fq[-1][1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fddac14",
   "metadata": {},
   "source": [
    "Note that the final number of bins is less than the state maximum. This is because bins that would have a probability below a certain threshold are combined together based on the precision of random values generated by random.uniform(), so that bins are impossible to be selected. Investigation into this precision is below (takes about 11 minutes to run on a lab client); in my experimentation, the smallest value returned was 4.416561560915966e-10, so MIN_FREQ >= 1e-10 should be sensible. Replace False in the below code block to test for yourself \n",
    "\n",
    "We choose to set the lower probability bound to 1e-9 for the sake of efficiency, and because we know that there are not enough unique values in the dataset to warrant more bins than a minimum probability of 1e-9 supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    smallest = 1\n",
    "    for i in range(int(1e9)): \n",
    "        smallest = min(smallest, random.uniform(0, 1))\n",
    "    smallest\n",
    "    del smallest, i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb8128da",
   "metadata": {},
   "source": [
    "We then test that this gives sensible outputs for a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b069a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame([[0, np.nan, 3],\n",
    "                       [1, np.nan, 3],\n",
    "                       [2, 5, np.nan],\n",
    "                       [3, 5, np.nan],\n",
    "                       ],\n",
    "                      columns=[\"A\", \"B\", \"C\"])\n",
    "print(df)\n",
    "print(\"\\n debug messages for generating A's frequency distribution:\")\n",
    "fq = get_fq(df, \"A\", 3, debugging=True)\n",
    "print()\n",
    "print(\"fq, as [probability, middle value] pairs: {}\".format(fq))\n",
    "print()\n",
    "print(\"value for probability 0.0: {}\".format(get_val_from_fq(fq, 0)))\n",
    "print(\"value for probability 0.5: {}\".format(get_val_from_fq(fq, 0.5)))\n",
    "print(\"value for probability 0.666: {}\".format(get_val_from_fq(fq, 0.666)))\n",
    "print(\"value for probability 1.0: {}\".format(get_val_from_fq(fq, 1)))\n",
    "\n",
    "del df, fq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f051d183",
   "metadata": {},
   "source": [
    "This appears to be sensible, though with such a small dataset the middle values are relatively further from the actual values than we hope they will be over large datasets.\n",
    "\n",
    "We next see how many bins are collapsed together when creating a frequency distribution of V1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_bins = 50\n",
    "got_bins = len(get_fq(credit_card, \"V1\", expected_bins, debugging=False))\n",
    "\n",
    "print(\"We expected {} but only got {} (bins with too low a probability are condensed together for efficiency).\".format(expected_bins, got_bins))\n",
    "\n",
    "del expected_bins, got_bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1ded71f",
   "metadata": {},
   "source": [
    "#### 4.3.4.2 Imputing from the Frequency Distribution\n",
    "\n",
    "Now that we can generate frequency distributions and get values from them, it is quite simple to create a new Imputer to fill in missing values in a given DataFrame. Note that the default max bin count of 50 is an arbitrary choice and warrants investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqDistImputer():\n",
    "    def __init__(self, verbose=0, max_bins=50):\n",
    "        self.verbose = verbose\n",
    "        self.max_bins = max_bins\n",
    "        self.freq_dists = {}\n",
    "\n",
    "    def fit(self, df: pandas.DataFrame):\n",
    "        for column in df.columns:\n",
    "            self.freq_dists[column] = get_fq(\n",
    "                df, column, self.max_bins, self.verbose)\n",
    "        if self.verbose:\n",
    "            print(self.freq_dists)\n",
    "\n",
    "    def transform(self, df: pandas.DataFrame):\n",
    "        df_imputed = df.copy()\n",
    "        na_rows, na_columns = np.where(pandas.isnull(df_imputed))\n",
    "        for row, col in (list(zip(na_rows, na_columns))):\n",
    "            index = df_imputed.index[row]\n",
    "            column_header = df_imputed.columns[col]\n",
    "            df_imputed.at[index, column_header] = get_val_from_fq(\n",
    "                self.freq_dists[column_header], random.uniform(0, 1))            \n",
    "        return df_imputed\n",
    "\n",
    "    def fit_transfrom(self, df: pandas.DataFrame): \n",
    "        self.fit(df)\n",
    "        self.transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d7fd182",
   "metadata": {},
   "source": [
    "Before using the binned frequency distribution function as an imputation method on the dataset, some testing is done to ensure it works as intended. Some elemental testing can therefore be found below. For future purposes, further testing encouragable, albeit it might potentially be difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbe356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame([\n",
    "    [1, np.nan, 3],\n",
    "    [np.nan, 4, 3],\n",
    "    [3, 4, 3],\n",
    "    [3, 4, 3],\n",
    "    [4, 5, np.nan]\n",
    "],\n",
    "    columns=[\"A\", \"B\", \"C\"]\n",
    ")\n",
    "print(df)\n",
    "imputer = FreqDistImputer(verbose=0)\n",
    "imputer.fit(df)\n",
    "df_imputed = imputer.transform(df)\n",
    "print(df_imputed)\n",
    "\n",
    "del df, df_imputed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8c2e3bb",
   "metadata": {},
   "source": [
    "### 4.1.4.3 Imputing the Binned Frequency Distribution\n",
    "\n",
    "Next, we can use the binned frequency distribution as our third imputation strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating the missing values, we can apply the binned frequency distribution imputation method\n",
    "# This will look similar to the procedures performed for the mean and median, with the exception of converting the array to a DataFrame\n",
    "binned_imputer = FreqDistImputer(verbose=0)\n",
    "credit_card_missing_values = remove_random_values(credit_card)\n",
    "\n",
    "binned_imputer.fit(credit_card_missing_values)\n",
    "imputed_frequency_df = binned_imputer.transform(credit_card_missing_values)\n",
    "\n",
    "imputed_frequency_df.info()\n",
    "\n",
    "del binned_imputer, credit_card_missing_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82f4dbe7",
   "metadata": {},
   "source": [
    "Having defined the DataFrame for a dataset containing missing values replaced with the binned frequency distribution imputation method, the strategy can be evaluated by comparing the means of the imputed binned frequnecy distribution values to the actual data's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage difference of the mean using the binned frequency distribution imputation\n",
    "percentage_diff_means = [(round(abs(imputed_mean - original_mean)/original_mean * 100, 2)) for (original_mean, imputed_mean)\n",
    " in zip(list(np.mean(credit_card, axis=0)), list(np.mean(imputed_frequency_df, axis=0)))]\n",
    "\n",
    "# We exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the mean for each column is {}%\".format(np.mean(percentage_diff_means[2:])))\n",
    "\n",
    "del percentage_diff_means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bbd2e05",
   "metadata": {},
   "source": [
    "As we can see, 21.32% is a substantial difference, indicating that the binned frequency distribution is not be the best measure for preserving the value of the means. It is also significantly higher than the results from both the mean and median imputation. Next, the impact on the median value was evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4efd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_diff_medians = []\n",
    "for (original_median, imputed_median) in zip(list(np.median(credit_card.drop(columns=[\"Class\"]), axis=0)), list(np.median(imputed_frequency_df.drop(columns=[\"Class\"]), axis=0))):\n",
    "    try:\n",
    "        percentage_diff_medians.append(round(exp(\n",
    "            log(abs(imputed_median - original_median)) - log(abs(original_median))) * 100, 2))\n",
    "        if (percentage_diff_medians[-1] > 0.5):\n",
    "            # print(\"big difference, original median: {}, imputed median: {}\".format(original_median, imputed_median))\n",
    "            pass \n",
    "    except:\n",
    "        percentage_diff_medians.append(0)\n",
    "\n",
    "\n",
    "# we exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the median for each column is {}%\".format(\n",
    "    np.mean(percentage_diff_medians[2:])))\n",
    "\n",
    "del percentage_diff_medians"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8511a72",
   "metadata": {},
   "source": [
    "Still, the binned frequency distribution imputation method is not performin very well as a method of replacing missing values in the dataset. With a mean impact on the median for each column of 12.34%, this imputation method is so far performing worse than both the mean and median imputation method. Next, we can compare the variance between the original dataset and the mean imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4118088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage difference in variance between the original and binned frequency distribution data\n",
    "percentage_diff_variances = [(round(abs(imputed_variance - original_variance)/original_variance * 100, 2)) for (original_variance, imputed_variance)\n",
    " in zip(list(np.var(credit_card, axis=0)), list(np.var(imputed_frequency_df, axis=0)))]\n",
    "\n",
    "# We exclude the first two columns because they are ID and time, which are not randomly nulled\n",
    "print(\"The mean impact on the variance for each column is {}%\".format(np.mean(percentage_diff_variances[2:])))\n",
    "\n",
    "del percentage_diff_variances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b803d06e",
   "metadata": {},
   "source": [
    "In comparing the variance, the binned frequency distribution performs more or less the same as the mean and median imputation methods, with a 5% differece. This is still considered a small significance considering the size of this dataset. Finally, we can comprare some values from the original and the imputed dataset graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dfaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at the median and mean for the imputed median\n",
    "print(np.median(credit_card[[\"V1\"]]))\n",
    "print(np.median(imputed_frequency_df[[\"V1\"]]))\n",
    "\n",
    "print(np.mean(credit_card[[\"V1\"]]))\n",
    "print(np.mean(imputed_frequency_df[[\"V1\"]]))\n",
    "\n",
    "# Explore the distribution graphically for some given columns\n",
    "fig, ax = plt.subplots()\n",
    "# Plot a scatter plot for the credit card data\n",
    "credit_card.plot.scatter(ax=ax, x=\"id\", y=\"V1\", color='blue', label='Credit Card')\n",
    "# Plot a scatter plot for the credit card data with the imputed median\n",
    "imputed_median_df.plot.scatter(ax=ax, x=\"id\", y=\"V1\", color='green', label='Imputed Median')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae430b94",
   "metadata": {},
   "source": [
    "### 4.2 Assessing Imputation Methods\n",
    "\n",
    "In assessing imputation methods, three models were tested. Replacing hypothetically missing values in the dataset using the mean, median and binned frequency distribution have different implications. One apparent finding from the comparison is how the median imputation method performs better than both the mean imputation method and the binned frequency distribution method, with the exception of evaluating the mean impact on the mean for each column.\n",
    "\n",
    "The **mean impact on the mean** is 14.16% for the median imputation, compared to -0.05% for the mean and 21.32% for the binned frequency distribution. While the **mean impact on the median** for each column is 0.60% for median imputation, in comparison, it is 50.16% for the mean imputation method and 12.34% for the binned frequency distribution method. Furthermore, the **mean impact on the variance** is 5.04% for the mean imputation method and 5.00% for the binned frequency distribution, while it is 4.99% percent for the median imputation method, which is slightly less.\n",
    "\n",
    "As such, the median imputation method would be the preferred way of replacing missing values for the dataset, in the event that these exist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f070c3c",
   "metadata": {},
   "source": [
    "# 5. Explore the Models \n",
    "The data has already been split into a training and test set, and for most of our models we can use K-fold cross validation, but it is too time-consuming to do so for some models, for which we will define a smaller training set and a validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d32e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_SEED = 42\n",
    "K = 5\n",
    "\n",
    "credit_card_X_under = credit_card_X_under.assign(Class=None)\n",
    "credit_card_X_under_clean = pandas.DataFrame(pipeline.fit_transform(credit_card_X_under), columns=[\n",
    "    item for item in credit_card.columns if item not in ['id', 'Class', 'Time']])\n",
    "credit_card_X_over = credit_card_X_over.assign(Class=None)\n",
    "credit_card_X_over_clean = pandas.DataFrame(pipeline.fit_transform(credit_card_X_over), columns=[\n",
    "    item for item in credit_card.columns if item not in ['id', 'Class', 'Time']])\n",
    "\n",
    "# Create our X and Y for K-fold validation\n",
    "credit_card_labels = credit_card[\"Class\"].copy()\n",
    "credit_card_X = pandas.DataFrame(credit_card_transformed, columns=[\n",
    "                                 item for item in credit_card.columns if item not in ['id', 'Class', 'Time']])\n",
    "\n",
    "credit_card_X_drop_uncorr = credit_card_X.drop(\n",
    "    columns=[item for item in uncorr_features if item not in ['id', 'Class', 'Time']])\n",
    "credit_card_X_drop_var = credit_card_X.drop(\n",
    "    columns=[item for item in low_variance_features if item not in ['id', 'Class', 'Time']])\n",
    "credit_card_X_distr = credit_card_X.drop(\n",
    "    columns=[item for item in similar_distribution_features if item not in ['id', 'Class', 'Time']])\n",
    "\n",
    "datasets = {\n",
    "    \"original\": (credit_card_X, credit_card_labels),\n",
    "    \"under\": (credit_card_X_under_clean, credit_card_labels_under),\n",
    "    \"over\": (credit_card_X_over_clean, credit_card_labels_over),\n",
    "    \"uncorr\": (credit_card_X_drop_uncorr, credit_card_labels),\n",
    "    \"low_var\": (credit_card_X_drop_var, credit_card_labels),\n",
    "    \"same_dist\": (credit_card_X_distr, credit_card_labels),\n",
    "}\n",
    "\n",
    "\n",
    "def split_data(x, y):\n",
    "    \"\"\"Create our testing and validation sets\"\"\"\n",
    "    shuffled_data = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=1/K, random_state=RANDOM_SEED)\n",
    "    [(train_index, validate_index)] = shuffled_data.split(x, y)\n",
    "    x_train = x.loc[train_index]\n",
    "    y_train = y.loc[train_index]\n",
    "    x_validate = x.loc[validate_index]\n",
    "    y_validate = y.loc[validate_index]\n",
    "    return x_train, y_train, x_validate, y_validate\n",
    "\n",
    "\n",
    "# Extract Xs and Ys from training and validation sets\n",
    "credit_card_X_train, credit_card_labels_train, credit_card_X_validate, credit_card_labels_validate = split_data(\n",
    "    credit_card_X, credit_card_labels)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"number of positives in training set: {len(list(filter(lambda x : x==1, credit_card_labels_train)))}\")\n",
    "print(\n",
    "    f\"number of positives in validation set: {len(list(filter(lambda x : x==1, credit_card_labels_validate)))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd548757",
   "metadata": {},
   "source": [
    "We next define some helper functions to see the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc542d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_matrix(y_true, y_pred):\n",
    "    \"\"\"Returns the TP, TN, FP and FN, i.e. the quadrants of a classification matrix for a binary classification problem.\"\"\"\n",
    "    cf_array = confusion_matrix(y_true, y_pred)\n",
    "    tp = cf_array[1][1]\n",
    "    tn = cf_array[0][0]\n",
    "    fp = cf_array[0][1]\n",
    "    fn = cf_array[1][0]\n",
    "    return (tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "def calc_metrics(tp, tn, fp, fn):\n",
    "    \"\"\"Returns the accuracy, precision, recall and F1 score from the TP, TN, FP and FN values.\"\"\"\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return (accuracy, precision, recall, f1)\n",
    "\n",
    "\n",
    "def stratified_K_fold(model, X: pandas.DataFrame, Y: pandas.Series, k=K, verbose=True, debugging=False):\n",
    "    \"\"\"Preforms stratified K-fold verification for a given model. Returns the mean accuracy, precision, recall and F1 score across folds.\"\"\"\n",
    "    skf = StratifiedKFold(k)\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(X, Y)):\n",
    "        # train the model on the training data\n",
    "        model.fit(X.iloc[train_index], Y.iloc[train_index])\n",
    "        # validate the model and calculate the desired metics\n",
    "        (tp, tn, fp, fn) = calc_matrix(\n",
    "            Y.iloc[valid_index], model.predict(X.iloc[valid_index]))\n",
    "        (accuracy, precision, recall, f1) = calc_metrics(tp, tn, fp, fn)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "        # print intermediate results\n",
    "        if verbose:\n",
    "            print(f\"fold {i}\")\n",
    "            print(f\"  accuracy:\\t{accuracy}\")\n",
    "            print(f\"  precision:\\t{precision}\")\n",
    "            print(f\"  recall:\\t{recall}\")\n",
    "            print(f\"  F1 score:\\t{f1}\")\n",
    "\n",
    "        # print debugging info\n",
    "        if debugging:\n",
    "            print(f\"train_index: {train_index}\")\n",
    "            print(f\"test_index: {valid_index}\")\n",
    "            print(\n",
    "                f\"number of positives in train set: {len(list(filter(lambda x : x==1, Y.iloc[train_index])))}\")\n",
    "            print(\n",
    "                f\"number of positives in validation set: {len(list(filter(lambda x : x==1, Y.iloc[valid_index])))}\")\n",
    "\n",
    "    return (np.mean(list(filter(lambda x: not np.isnan(x), accuracies))),\n",
    "            np.mean(list(filter(lambda x: not np.isnan(x), precisions))),\n",
    "            np.mean(list(filter(lambda x: not np.isnan(x), recalls))),\n",
    "            np.mean(list(filter(lambda x: not np.isnan(x), f1s))),\n",
    "            )\n",
    "\n",
    "\n",
    "def stratified_K_fold_scores(model, X: pandas.DataFrame, Y: pandas.Series, k=K, verbose=True, debugging=False):\n",
    "    \"\"\"Performs stratified K-fold verification for a given model and prints the mean accuracy, precision, recall and F1 score across folds.\n",
    "       \\nN.B. warnings are ignored due to the high likelihood of division-by-zero warnings.\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy, precision, recall, f1 = stratified_K_fold(\n",
    "            model, X, Y, k, verbose, debugging)\n",
    "        print()\n",
    "        print(f\"Mean accuracy:\\t{accuracy}\")\n",
    "        print(f\"Mean precision:\\t{precision}\")\n",
    "        print(f\"Mean recall:\\t{recall}\")\n",
    "        print(f\"Mean F1 score:\\t{f1}\")\n",
    "\n",
    "\n",
    "def stratified_K_fold_scores_datasets(model, k=K, verbose=False, debugging=False):\n",
    "    \"\"\"Performs stratified K-fold verification for a given model on all datasets.\"\"\"\n",
    "    for (name, x_y) in datasets.items():\n",
    "        print(name, end='')\n",
    "        stratified_K_fold_scores(model, x_y[0], x_y[1], k, verbose, debugging)\n",
    "        print()\n",
    "\n",
    "\n",
    "def validation_set_scores(model, train_x, train_y, validate_x, validate_y, already_fit=False):\n",
    "    \"\"\"Prints the performance metrics for a model that uses a training and validation set instead of K-fold validation.       \n",
    "    \\nN.B. warnings are ignored due to the high likelihood of division-by-zero warnings.\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if not already_fit:\n",
    "            model.fit(train_x, train_y)\n",
    "        (tp, tn, fp, fn) = calc_matrix(validate_y, model.predict(validate_x))\n",
    "        (accuracy, precision, recall, f1) = calc_metrics(tp, tn, fp, fn)\n",
    "        print(f\"Accuracy:\\t{accuracy}\")\n",
    "        print(f\"Precision:\\t{precision}\")\n",
    "        print(f\"Recall:\\t\\t{recall}\")\n",
    "        print(f\"F1 score:\\t{f1}\")\n",
    "\n",
    "\n",
    "def validation_set_scores_datasets(model, already_fit=False):\n",
    "    \"\"\"Prints the performance metrics for a model that uses a training and validation set instead of K-fold validation.       \n",
    "    \\nN.B. warnings are ignored due to the high likelihood of division-by-zero warnings.\"\"\"\n",
    "    for (name, x_y) in datasets.items():\n",
    "        print(name)\n",
    "        x_train, y_train, x_validate, y_validate = split_data(x_y[0], x_y[1])\n",
    "        validation_set_scores(model, x_train, y_train,\n",
    "                              x_validate, y_validate, already_fit)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5da92",
   "metadata": {},
   "source": [
    "\n",
    "As a baseline to compare against, we will just predict class = 0 for all inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "stratified_K_fold_scores(dummy_classifier, credit_card_X, credit_card_labels)\n",
    "\n",
    "del dummy_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebb0835a",
   "metadata": {},
   "source": [
    "As one might expect, this has a very high accuracy, but it has a recall of zero and the precision cannot be calculated as there are neither true positives nor false positives. We can use the dummy to demonstrate that of our datasets, only under and oversampling have changed the ratio of positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fe93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "stratified_K_fold_scores_datasets(dummy_classifier)\n",
    "\n",
    "del dummy_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc7a04d4",
   "metadata": {},
   "source": [
    "## 5.1 Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50baf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = SGDClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "stratified_K_fold_scores(logistic_classifier, credit_card_X, credit_card_labels)\n",
    "\n",
    "del logistic_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0ae78e",
   "metadata": {},
   "source": [
    "Our linear classifier only classifies any transactions as fraudulent for our first fold, so our precision and recall is NaN for the other folds - they are the same as the dummy classifier, only much more computationally expensive. Our first fold has a lower accuracy and its precision is zero becase the instances that it classifies as fraudulent are not fraudulent, so the average accuracy of the linear classifier is actually worse than the dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba901e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = SGDClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "stratified_K_fold_scores_datasets(logistic_classifier)\n",
    "\n",
    "del logistic_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fd6b59c",
   "metadata": {},
   "source": [
    "Under and oversampling massively improve the recall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9c25ea2",
   "metadata": {},
   "source": [
    "## 5.2 Logisitic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b87f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = SGDClassifier(loss='log_loss', random_state=RANDOM_SEED)\n",
    "\n",
    "stratified_K_fold_scores(logistic_classifier, credit_card_X, credit_card_labels)\n",
    "\n",
    "del logistic_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175b2e9e",
   "metadata": {},
   "source": [
    "The logistic classifier appears to suffer from the same problems as the linear model, although its has fewer false positives in the first fold. Its performance is still worse than the dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c351389",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = SGDClassifier(loss='log_loss', random_state=RANDOM_SEED)\n",
    "\n",
    "stratified_K_fold_scores_datasets(logistic_classifier)\n",
    "\n",
    "del logistic_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8140cda1",
   "metadata": {},
   "source": [
    "Again, under and oversampling improve recall, but less than for the linear classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1599b41d",
   "metadata": {},
   "source": [
    "## 5.3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f430273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "stratified_K_fold_scores(tree_classifier, credit_card_X, credit_card_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99029a0b",
   "metadata": {},
   "source": [
    "Our decision tree model is much less conservative with positive classifications, and has significantly increased our recall score (though it is still very low, missing the majority of fraudulent transactions). Despite the accuracy being lower than the dummy classifier, this is the most promising model so far. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fa39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "stratified_K_fold_scores_datasets(tree_classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7350cd00",
   "metadata": {},
   "source": [
    "Under sampling has obtained a recall of 1, and an F1 score of 0.97. This is likely to be our most promising model and dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69415ff6",
   "metadata": {},
   "source": [
    "## 5.4 Random Forest Classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd91846",
   "metadata": {},
   "source": [
    "Random forest classifiers are slow to train, so K-fold validation is not feasible in our timeframe. Instead, we use the training and validation sets we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier = RandomForestClassifier(random_state = RANDOM_SEED)\n",
    "validation_set_scores(forest_classifier, credit_card_X_train, credit_card_labels_train, credit_card_X_validate, credit_card_labels_validate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdb0552a",
   "metadata": {},
   "source": [
    "This is worse than the dummy classifier was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier = RandomForestClassifier(random_state = RANDOM_SEED)\n",
    "validation_set_scores_datasets(forest_classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ce74a6",
   "metadata": {},
   "source": [
    "### 5.4.2 Plotting ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross_val_predict() function, perform K-folder cross-validation to return a prediction for each fold (Aurelien Geron, 2020)\n",
    "y_probs_forest = cross_val_predict(forest_classifier, credit_card_X, credit_card_labels.values.ravel(), cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the data\n",
    "y_probs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probs_forest[:, 1]\n",
    "y_scores_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TPR and FPR of various thresholds (Aurelien Geron, 2020)\n",
    "FPR_forest, TPR_forest, thresholds_forest = roc_curve(credit_card_labels, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400103c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the lowest threshold that provides at least 90 percent accuracy (Aurelien Geron, 2020)\n",
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(credit_card_labels, y_scores_forest)\n",
    "index_of_first_precision_at_least_90_percent_forest = np.argmax(precisions_forest >= 0.90)\n",
    "recall_for_90_percent_precision_forest = recalls_forest[index_of_first_precision_at_least_90_percent_forest]\n",
    "FPR_for_90_percent_precision_forest = FPR_forest[np.argmax(TPR_forest >= recall_for_90_percent_precision_forest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the curve of FPR to TPR (Aurelien Geron, 2020)\n",
    "def plot_roc_curve(FPR, TPR, label=None): \n",
    "    plt.plot(FPR, TPR, linewidth=2, label=label) \n",
    "    plt.plot([0, 1], [0, 1], 'k--') \n",
    "    plt.axis([0, 1, 0, 1]) \n",
    "    plt.xlabel('FPR (1 - specificity)', fontsize=16) \n",
    "    plt.ylabel('TPR (recall)', fontsize=16) \n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the roc curves\n",
    "plt.figure(figsize=(8, 6)) \n",
    "\n",
    "plot_roc_curve(FPR_forest, TPR_forest, \"Random Forest\") \n",
    "plt.plot([FPR_for_90_percent_precision_forest],[recall_for_90_percent_precision_forest], \"ro\")\n",
    "plt.grid(True) \n",
    "plt.legend(loc=\"lower right\", fontsize=16) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80883f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate roc_auc_score\n",
    "roc_auc_score(credit_card_labels, y_scores_forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ae23a87",
   "metadata": {},
   "source": [
    "### 5.4.2 Dropping non-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcab992",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_X_drop_uncorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbe303",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier_2 = RandomForestClassifier(n_estimators = 100, random_state = 42, oob_score=True)\n",
    "\n",
    "forest_classifier_2.fit(credit_card_X_drop_uncorr, credit_card_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_predictions_2 = forest_classifier_2.predict(credit_card_X_drop_uncorr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(credit_card_labels, credit_card_predictions_2)\n",
    "matrix\n",
    "\n",
    "# 1 fraudulent transaction was not detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the confusion matrix\n",
    "sns.heatmap(matrix, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision_score \n",
    "precision_score(credit_card_labels, credit_card_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall_score\n",
    "recall_score(credit_card_labels, credit_card_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201421ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(credit_card_labels, credit_card_predictions_2)\n",
    "\n",
    "#Not much change from the original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea272353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest = cross_val_predict(forest_classifier_2, credit_card_X_drop_uncorr, credit_card_labels.values.ravel(), cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probs_forest[:, 1]\n",
    "y_scores_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5160b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR_forest, TPR_forest, thresholds_forest = roc_curve(credit_card_labels, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47025576",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(credit_card_labels, y_scores_forest)\n",
    "index_of_first_precision_at_least_90_percent_forest = np.argmax(precisions_forest >= 0.90)\n",
    "recall_for_90_percent_precision_forest = recalls_forest[index_of_first_precision_at_least_90_percent_forest]\n",
    "FPR_for_90_percent_precision_forest = FPR_forest[np.argmax(TPR_forest >= recall_for_90_percent_precision_forest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6)) \n",
    "\n",
    "plot_roc_curve(FPR_forest, TPR_forest, \"Random Forest\") \n",
    "plt.plot([FPR_for_90_percent_precision_forest],[recall_for_90_percent_precision_forest], \"ro\")\n",
    "plt.grid(True) \n",
    "plt.legend(loc=\"lower right\", fontsize=16) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed830a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(credit_card_labels, y_scores_forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b153968",
   "metadata": {},
   "source": [
    "### 5.4.3 Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random undersampling to balance the Class distribution\n",
    " \n",
    "X = credit_card\n",
    "y = credit_card[[\"Class\"]]\n",
    "\n",
    "\n",
    "# Define undersample strategy\n",
    "# There are 218,660 examples in the majority class and 469 examples in the minority class\n",
    "# After undersampling, both classes have 469 examples in the transformed training data set\n",
    "\n",
    "undersample_02 = RandomUnderSampler(sampling_strategy='majority')\n",
    "#undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "credit_card_3_X, credit_card_3_y = undersample_02.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6fe537",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_3_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abe672",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_3_X = credit_card_3_X.drop(columns = [ \n",
    "                                            'Class',\n",
    "                                            'id',\n",
    "                                            'Time',\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_3_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_3_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier_3 = RandomForestClassifier(\n",
    "    n_estimators=100, random_state=42, oob_score=True)\n",
    "\n",
    "forest_classifier_3.fit(credit_card_3_X, credit_card_3_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_predictions_3 = forest_classifier_3.predict(credit_card_3_X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(credit_card_3_y, credit_card_predictions_3)\n",
    "matrix\n",
    "\n",
    "# All detected perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(credit_card_3_y, credit_card_predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf45f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(credit_card_3_y, credit_card_predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(credit_card_3_y, credit_card_predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24377ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest = cross_val_predict(forest_classifier_3, credit_card_3_X, credit_card_3_y.values.ravel(), cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93267f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded array\n",
    "y_scores_forest = y_probs_forest[:, 1]\n",
    "y_scores_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9da548",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR_forest, TPR_forest, thresholds_forest = roc_curve(credit_card_3_y, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(credit_card_3_y, y_scores_forest)\n",
    "index_of_first_precision_at_least_90_percent_forest = np.argmax(precisions_forest >= 0.90)\n",
    "recall_for_90_percent_precision_forest = recalls_forest[index_of_first_precision_at_least_90_percent_forest]\n",
    "FPR_for_90_percent_precision_forest = FPR_forest[np.argmax(TPR_forest >= recall_for_90_percent_precision_forest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6)) \n",
    "\n",
    "plot_roc_curve(FPR_forest, TPR_forest, \"Random Forest\") \n",
    "plt.plot([FPR_for_90_percent_precision_forest],[recall_for_90_percent_precision_forest], \"ro\")\n",
    "plt.grid(True) \n",
    "plt.legend(loc=\"lower right\", fontsize=16) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ca0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc_auc_score\n",
    "roc_auc_score(credit_card_3_y, y_scores_forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27f93df6",
   "metadata": {},
   "source": [
    "### 5.4.4 Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb926a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversampling to balance the class distribution\n",
    "X = credit_card\n",
    "y = credit_card[[\"Class\"]]\n",
    "\n",
    "# Define oversample strategy\n",
    "# This strategy oversamples the minority class to a total of 218,660 examples\n",
    "oversample_minority = RandomOverSampler(sampling_strategy=0.25)\n",
    "# Fit\n",
    "credit_card_4_X, credit_card_4_y = oversample_minority.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257aabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_4_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_4_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier_4 = RandomForestClassifier(n_estimators = 100, random_state = 42, oob_score=True)\n",
    "\n",
    "forest_classifier_4.fit(credit_card_4_X, credit_card_4_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_predictions_4 = forest_classifier_4.predict(credit_card_4_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(credit_card_4_y, credit_card_predictions_4)\n",
    "matrix\n",
    "\n",
    "#All detected perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1341656",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(credit_card_4_y, credit_card_predictions_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b59cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(credit_card_4_y, credit_card_predictions_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(credit_card_4_y, credit_card_predictions_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest = cross_val_predict(forest_classifier_4, credit_card_4_X, credit_card_4_y.values.ravel(), cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c19e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probs_forest[:, 1]\n",
    "y_scores_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR_forest, TPR_forest, thresholds_forest = roc_curve(credit_card_4_y, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fe583",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(credit_card_4_y, y_scores_forest)\n",
    "index_of_first_precision_at_least_90_percent_forest = np.argmax(precisions_forest >= 0.90)\n",
    "recall_for_90_percent_precision_forest = recalls_forest[index_of_first_precision_at_least_90_percent_forest]\n",
    "FPR_for_90_percent_precision_forest = FPR_forest[np.argmax(TPR_forest >= recall_for_90_percent_precision_forest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe53ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plot_roc_curve(FPR_forest, TPR_forest, \"Random Forest\")\n",
    "plt.plot([FPR_for_90_percent_precision_forest], [\n",
    "         recall_for_90_percent_precision_forest], \"ro\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(credit_card_4_y, y_scores_forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd04583b",
   "metadata": {},
   "source": [
    "#### 5.4.5 Dropping the features that have the same distribution (between genuine and fraudulent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f758cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_X_distr = credit_card.drop(columns=[\n",
    "    'Class',\n",
    "    'id',\n",
    "    'Time',\n",
    "\n",
    "    'V5',\n",
    "    'V6',\n",
    "    'V7',\n",
    "    'V8',\n",
    "    'V12',\n",
    "    'V13',\n",
    "    'V15',\n",
    "    'V16',\n",
    "    'V20',\n",
    "    'V21',\n",
    "    'V22',\n",
    "    'V23',\n",
    "    'V25',\n",
    "    'V27',\n",
    "    'V28',\n",
    "\n",
    "    'Amount',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier_6 = RandomForestClassifier(\n",
    "    n_estimators=100, random_state=42, oob_score=True)\n",
    "\n",
    "forest_classifier_6.fit(credit_card_X_distr, credit_card_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_predictions_6 = forest_classifier_6.predict(credit_card_X_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53650998",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(credit_card_labels, credit_card_predictions_6)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c29fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(credit_card_labels, credit_card_predictions_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(credit_card_labels, credit_card_predictions_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11294dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(credit_card_labels, credit_card_predictions_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416474f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_forest = cross_val_predict(forest_classifier_6, credit_card_X_distr, credit_card_labels.values.ravel(), cv=3, method=\"predict_proba\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8012869",
   "metadata": {},
   "source": [
    "y_probs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probs_forest[:, 1]\n",
    "y_scores_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR_forest, TPR_forest, thresholds_forest = roc_curve(credit_card_labels, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5261876",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(credit_card_labels, y_scores_forest)\n",
    "index_of_first_precision_at_least_80_percent_forest = np.argmax(precisions_forest >= 0.80)\n",
    "recall_for_80_percent_precision_forest = recalls_forest[index_of_first_precision_at_least_80_percent_forest]\n",
    "FPR_for_80_percent_precision_forest = FPR_forest[np.argmax(TPR_forest >= recall_for_80_percent_precision_forest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6)) \n",
    "\n",
    "plot_roc_curve(FPR_forest, TPR_forest, \"Random Forest\") \n",
    "plt.plot([FPR_for_90_percent_precision_forest],[recall_for_80_percent_precision_forest], \"ro\")\n",
    "plt.grid(True) \n",
    "plt.legend(loc=\"lower right\", fontsize=16) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29260509",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(credit_card_labels, y_scores_forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a1789cc",
   "metadata": {},
   "source": [
    "### 5.4.6 Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ed02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation  \n",
    "# It takes a lot of time - 8 hours\n",
    "\n",
    "K_ = 10\n",
    "# Split the training set into 10 different subsets(Aurelien Geron, 2020)\n",
    "# Reference: https://github.com/ageron/handson-ml2\n",
    "\n",
    "\n",
    "forest_classifier_scores = cross_val_score(forest_classifier,\n",
    "                                           credit_card_X, \n",
    "                                           credit_card_labels,\n",
    "                                           scoring=\"neg_mean_squared_error\", \n",
    "                                           cv=K_)\n",
    "\n",
    "forest_classifier_rmse_scores = np.sqrt(-forest_classifier_scores) \n",
    "                                           #notes the minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\",             np.round(scores))\n",
    "    print(\"Mean:\",               np.round(scores.mean())) \n",
    "    print(\"Standard deviation:\", np.round(scores.std()))\n",
    "\n",
    "\n",
    "display_scores(forest_classifier_rmse_scores) \n",
    "\n",
    "# All zero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb0d8c4e",
   "metadata": {},
   "source": [
    "## 5.5 Support Vector Classifier (SVC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca01d3e6",
   "metadata": {},
   "source": [
    "### 5.5.1 Poly Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_kernel_svc = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "validation_set_scores(poly_kernel_svc, credit_card_X_train, credit_card_labels_train, credit_card_X_validate, credit_card_labels_validate)\n",
    "\n",
    "del poly_kernel_svc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c34b7e2",
   "metadata": {},
   "source": [
    "### 5.5.2 Gaussian Radial Bias Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svc = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "validation_set_scores(rbf_kernel_svc, credit_card_X_train, credit_card_labels_train, credit_card_X_validate, credit_card_labels_validate)\n",
    "\n",
    "\n",
    "del rbf_kernel_svc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a664e8",
   "metadata": {},
   "source": [
    "We do not have time to try the SVCs on all datasets. They do not appear to outperform random forest or decision tree based on preliminary testing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9198c60f",
   "metadata": {},
   "source": [
    "## 5.6 Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "validation_set_scores_datasets(gnb)\n",
    "\n",
    "del gnb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ef5c571",
   "metadata": {},
   "source": [
    "# 6 Tuning the Model\n",
    "## 6.1 Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98645dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [\n",
    "  \n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "\n",
    "forest_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42, oob_score=True)\n",
    "\n",
    "print(forest_classifier.feature_importance_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3779b487",
   "metadata": {},
   "source": [
    "We can reduce our features based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on forest_classifier.feature_importance_\n",
    "unimportant = ['Class','id','TIme','V1','V5','V6','V7','V11','V12','V13','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V27','V28','Amount']\n",
    "\n",
    "grid_search = GridSearchCV(forest_classifier, parameter_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(credit_card.drop(columns=unimportant), credit_card_labels)\n",
    "\n",
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9519bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results = grid_search.cv_results_\n",
    "for mean_score, params in zip(grid_search_results[\"mean_test_score\"], \n",
    "                              grid_search_results[\"params\"]):\n",
    "    print(np.round(np.sqrt(-mean_score)), params)\n",
    "    \n",
    "    # Not working\n",
    "    # Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef73a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_test = pandas.read_csv(\"../data/test.csv\")\n",
    "credit_card_y_predicted = forest_classifier.predict_proba(credit_card_test)[:,1]\n",
    "# Predict_proba: calculate the class probability of each row of credit card dataï¼Giorgos Myrianthous, 2021ï¼\n",
    "# Referenceï¼https://towardsdatascience.com/predict-vs-predict-proba-scikit-learn-bdc45daa5972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = credit_card = pandas.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a12069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Class'] = [x[1] for x in credit_card_y_predicted]\n",
    "sample_submission.to_csv('Group17_forest.csv', index=False)   \n",
    "# (pandas.DataFrame.to_csv)\n",
    "# Reference:https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier stratified k-cross validation\n",
    "\n",
    "credit_card_labels = credit_card[\"Class\"].copy()\n",
    "credit_card_X = credit_card.drop(columns=[\"id\", \"Class\"])\n",
    "\n",
    "stratified_K_fold_scores(forest_classifier, credit_card_X, credit_card_labels)\n",
    "\n",
    "del forest_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "203aadba",
   "metadata": {},
   "source": [
    "## 6.2 Decision Tree Grid Search\n",
    "We found the decision tree with the undersampled dataset to be our best model, and is quicker to compute than an equivalent random forest, so we will investigate tuning it. First, we see what values were chosen by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=RANDOM_SEED)\n",
    "tree_classifier.fit(credit_card_X_under_clean, credit_card_labels_under)\n",
    "\n",
    "print(tree_classifier.get_params())\n",
    "\n",
    "del tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39400882",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'random_state': [12, 24, RANDOM_SEED, 5059],\n",
    "    'max_depth': [5, 25, 40, 50],\n",
    "    'min_samples_split': [1, 2, 5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['random', 'best'],\n",
    "}\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=tree_classifier, param_grid=param_grid, cv=K, scoring='recall')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    grid_search.fit(credit_card_X_under_clean, credit_card_labels_under)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best recall: {grid_search.best_score_}\")\n",
    "\n",
    "del param_grid, tree_classifier, grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b523b",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2.1 Testing with Kaggle\n",
    "Now that we have tuned our model, we can test its predictions on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test dataset\n",
    "credit_card_test = pandas.read_csv(\"../data/test.csv\")\n",
    "\n",
    "credit_card_test = credit_card_test.assign(Class=None)\n",
    "\n",
    "credit_card_test_clean = pandas.DataFrame(pipeline.fit_transform(credit_card_test), columns=[\n",
    "    item for item in credit_card.columns if item not in ['id', 'Class', 'Time']])\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
    "                                         max_depth=5, max_features='sqrt', min_samples_split=10, random_state=12, splitter='random')\n",
    "tree_classifier.fit(credit_card_X_under_clean, credit_card_labels_under)\n",
    "\n",
    "# Create predictions on test data\n",
    "predictions = tree_classifier.predict(credit_card_test_clean)\n",
    "\n",
    "print(\"Decision Tree Original Predictions:\", predictions)\n",
    "\n",
    "# Test the probabilities for the test data using the decision tree classifier\n",
    "probability = tree_classifier.predict_proba(credit_card_test_clean)\n",
    "\n",
    "print(\"Decision Tree Original Probabilities:\", probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pandas.read_csv(\"../data/sample_submission.csv\")\n",
    "sample_submission_pos = sample_submission[[\"id\"]].assign(Class = [x[1] for x in probability])\n",
    "sample_submission_pos.to_csv('Group_17_positive.csv', index=False)   \n",
    "sample_submission_neg = sample_submission[[\"id\"]].assign(Class = [x[0] for x in probability])\n",
    "sample_submission_neg.to_csv('Group_17_negative.csv', index=False)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
